{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5a690f",
   "metadata": {},
   "source": [
    "**Ejercicio 1**: Datos no Estructurados, clasificación de documentos en formato imagen de Davivienda.\n",
    "\n",
    "**Por**: *David Doncel Ballén*, creado en agosto 7 de 2022.\n",
    "\n",
    "**Code Repository**: https://github.com/DaDo82/Machine-Learning/tree/main/Ej1_clasificador_doc_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189d883",
   "metadata": {},
   "source": [
    "# Ejercicio 1 - Clasificación de documentos con o sin contenido en formato imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293bd7d5",
   "metadata": {},
   "source": [
    "El objetivo de este ejercicio es clasificar hojas de documentos con o sin contenido que se encuentran en formato imagen implementando una red neuronal artificial multicapa desde cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31941583",
   "metadata": {},
   "source": [
    "# Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899482ff",
   "metadata": {},
   "source": [
    "- [Cargue de las Imágenes](#Cargue-de-las-imágenes)\n",
    "- [Limpieza de los datos](#Limpieza-de-los-datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4c930",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5593ceb",
   "metadata": {},
   "source": [
    "# Cargue de las Imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d751bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "# importing the library\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5bbaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo imagenes de  D:\\Data_Science\\Pruebas\\Davivienda\\Ejercicio_1_Imagenes\\Notebook\\Datasets\\\n",
      "D:\\Data_Science\\Pruebas\\Davivienda\\Ejercicio_1_Imagenes\\Notebook\\Datasets\\Blanco 1\n",
      "D:\\Data_Science\\Pruebas\\Davivienda\\Ejercicio_1_Imagenes\\Notebook\\Datasets\\Documentos 147\n",
      "Directorios leidos: 2\n",
      "Imagenes en cada directorio [148, 99]\n",
      "suma Total de imagenes en subdirs: 247\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.join(os.getcwd(), 'Datasets')\n",
    "imgpath = dirname + os.sep \n",
    "\n",
    "images = []\n",
    "directories = []\n",
    "dircount = []\n",
    "prevRoot=''\n",
    "cant=0\n",
    "\n",
    "print(\"leyendo imagenes de \",imgpath)\n",
    "\n",
    "for root, dirnames, filenames in os.walk(imgpath):\n",
    "    for filename in filenames:\n",
    "        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
    "            cant=cant+1\n",
    "            filepath = os.path.join(root, filename)\n",
    "            image = plt.imread(filepath)\n",
    "            images.append(image)\n",
    "            b = \"Leyendo...\" + str(cant)\n",
    "            print (b, end=\"\\r\")\n",
    "            if prevRoot !=root:\n",
    "                print(root, cant)\n",
    "                prevRoot=root\n",
    "                directories.append(root)\n",
    "                dircount.append(cant)\n",
    "                cant=0\n",
    "dircount.append(cant)\n",
    "\n",
    "dircount = dircount[1:]\n",
    "dircount[0]=dircount[0]+1\n",
    "print('Directorios leidos:',len(directories))\n",
    "print(\"Imagenes en cada directorio\", dircount)\n",
    "print('suma Total de imagenes en subdirs:',sum(dircount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ab7c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad etiquetas creadas:  247\n",
      "0 Blanco\n",
      "1 Documentos\n",
      "Total number of outputs :  2\n",
      "Output classes :  [0 1]\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "indice=0\n",
    "for cantidad in dircount:\n",
    "    for i in range(cantidad):\n",
    "        labels.append(indice)\n",
    "    indice=indice+1\n",
    "print(\"Cantidad etiquetas creadas: \",len(labels))\n",
    "\n",
    "deportes=[]\n",
    "indice=0\n",
    "for directorio in directories:\n",
    "    name = directorio.split(os.sep)\n",
    "    print(indice , name[len(name)-1])\n",
    "    deportes.append(name[len(name)-1])\n",
    "    indice=indice+1\n",
    "    \n",
    "y = np.array(labels)\n",
    "X = np.array(images, dtype=np.uint8)\n",
    "#X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    "\n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec708ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (197, 400, 250, 3) (197,)\n",
      "Testing data shape :  (50, 400, 250, 3) (50,)\n",
      "Original label: 0\n",
      "After conversion to one-hot: [1. 0.]\n",
      "(157, 400, 250, 3) (40, 400, 250, 3) (157, 2) (40, 2)\n"
     ]
    }
   ],
   "source": [
    "#Mezclar todo y crear los grupos de entrenamiento y testing\n",
    "train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.2)\n",
    "print('Training data shape : ', train_X.shape, train_Y.shape)\n",
    "print('Testing data shape : ', test_X.shape, test_Y.shape)\n",
    "\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255.\n",
    "\n",
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])\n",
    "\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)\n",
    "\n",
    "print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e76a83",
   "metadata": {},
   "source": [
    "# Al cambiar el número de ventanas de 8 a 16 y el tamaño de kernel de 3,7 a 5 tiene mejor rendimiento y ajuste la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b65d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 400, 250, 16)      1216      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 400, 250, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 200, 125, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200, 125, 16)      0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400000)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                6400016   \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,401,266\n",
      "Trainable params: 6,401,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davze\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adagrad.py:77: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adagrad, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(16, kernel_size=(5, 5),activation='linear',padding='same',input_shape=(400,250,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "\n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(16, activation='linear'))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5)) \n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "sport_model.summary()\n",
    "\n",
    "sport_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 10s 140ms/step - loss: 1.7674 - accuracy: 0.5096 - val_loss: 0.6841 - val_accuracy: 0.5750\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.8938 - accuracy: 0.5350 - val_loss: 0.6928 - val_accuracy: 0.5750\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.8065 - accuracy: 0.5478 - val_loss: 0.6785 - val_accuracy: 0.5750\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.8474 - accuracy: 0.4713 - val_loss: 0.7130 - val_accuracy: 0.5750\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.7355 - accuracy: 0.5541 - val_loss: 0.6883 - val_accuracy: 0.5750\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.8099 - accuracy: 0.5159 - val_loss: 0.6699 - val_accuracy: 0.5750\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.6933 - accuracy: 0.5287 - val_loss: 0.9092 - val_accuracy: 0.5750\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.7832 - accuracy: 0.4968 - val_loss: 0.6713 - val_accuracy: 0.7250\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.6778 - accuracy: 0.5732 - val_loss: 0.6633 - val_accuracy: 0.5750\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.6707 - accuracy: 0.5732 - val_loss: 0.6598 - val_accuracy: 0.5750\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.7210 - accuracy: 0.5350 - val_loss: 0.6518 - val_accuracy: 0.5750\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.6621 - accuracy: 0.5732 - val_loss: 0.6747 - val_accuracy: 0.4250\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.6543 - accuracy: 0.6242 - val_loss: 0.6472 - val_accuracy: 0.7250\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.6153 - accuracy: 0.6815 - val_loss: 0.6662 - val_accuracy: 0.4250\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.6429 - accuracy: 0.6497 - val_loss: 0.6391 - val_accuracy: 0.7250\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.6207 - accuracy: 0.6178 - val_loss: 0.6522 - val_accuracy: 0.4250\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.6030 - accuracy: 0.7197 - val_loss: 0.5916 - val_accuracy: 0.6000\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.7078 - accuracy: 0.7006 - val_loss: 0.6237 - val_accuracy: 0.6750\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.6353 - accuracy: 0.6306 - val_loss: 0.6235 - val_accuracy: 0.7500\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.5812 - accuracy: 0.7070 - val_loss: 0.6402 - val_accuracy: 0.9750\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.5579 - accuracy: 0.7771 - val_loss: 0.5454 - val_accuracy: 0.8750\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.4840 - accuracy: 0.8217 - val_loss: 0.4465 - val_accuracy: 0.8000\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.6358 - accuracy: 0.7580 - val_loss: 0.5691 - val_accuracy: 0.8000\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.5543 - accuracy: 0.7834 - val_loss: 0.5822 - val_accuracy: 0.9750\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.5238 - accuracy: 0.8025 - val_loss: 0.4680 - val_accuracy: 0.7250\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.3542 - accuracy: 0.8790 - val_loss: 0.3763 - val_accuracy: 0.8750\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.3487 - accuracy: 0.8599 - val_loss: 0.5682 - val_accuracy: 0.6750\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.3368 - accuracy: 0.8917 - val_loss: 0.6767 - val_accuracy: 0.6500\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.2474 - accuracy: 0.9363 - val_loss: 0.3588 - val_accuracy: 0.8250\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.3100 - accuracy: 0.8790 - val_loss: 0.3239 - val_accuracy: 0.8750\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.2040 - accuracy: 0.9554 - val_loss: 0.3817 - val_accuracy: 0.8000\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.1937 - accuracy: 0.9490 - val_loss: 0.3777 - val_accuracy: 0.8000\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.1634 - accuracy: 0.9490 - val_loss: 0.3159 - val_accuracy: 0.8750\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.1436 - accuracy: 0.9809 - val_loss: 0.3281 - val_accuracy: 0.8750\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.1548 - accuracy: 0.9618 - val_loss: 0.2975 - val_accuracy: 0.9000\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.1557 - accuracy: 0.9682 - val_loss: 0.3162 - val_accuracy: 0.9000\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.1286 - accuracy: 0.9809 - val_loss: 0.3660 - val_accuracy: 0.8250\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.1363 - accuracy: 0.9554 - val_loss: 0.3487 - val_accuracy: 0.8500\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.1155 - accuracy: 0.9809 - val_loss: 0.3364 - val_accuracy: 0.8750\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.1299 - accuracy: 0.9554 - val_loss: 0.3143 - val_accuracy: 0.9000\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.1091 - accuracy: 0.9809 - val_loss: 0.2856 - val_accuracy: 0.9000\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.1532 - accuracy: 0.9618 - val_loss: 0.3525 - val_accuracy: 0.8250\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.1536 - accuracy: 0.9809 - val_loss: 0.3316 - val_accuracy: 0.8750\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.1156 - accuracy: 0.9809 - val_loss: 0.3172 - val_accuracy: 0.9000\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.1402 - accuracy: 0.9809 - val_loss: 0.3205 - val_accuracy: 0.9000\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.1100 - accuracy: 0.9745 - val_loss: 0.3157 - val_accuracy: 0.9000\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9745"
     ]
    }
   ],
   "source": [
    "sport_train = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    "\n",
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "sport_model.save(\"docs_modelClass.h5py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = sport_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
    "\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sport_train.history['accuracy']\n",
    "val_accuracy = sport_train.history['val_accuracy']\n",
    "loss = sport_train.history['loss']\n",
    "val_loss = sport_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d442502",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes2 = sport_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes=[]\n",
    "for predicted_sport in predicted_classes2:\n",
    "    predicted_classes.append(predicted_sport.tolist().index(max(predicted_sport)))\n",
    "predicted_classes=np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6433cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ba06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.where(predicted_classes==test_Y)[0]\n",
    "print(\"Found %d correct labels\" % len(correct))\n",
    "for i, correct in enumerate(correct[0:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_X[correct].reshape(400,250,3), cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}, {}\".format(deportes[predicted_classes[correct]],\n",
    "                                                    deportes[test_Y[correct]]))\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = np.where(predicted_classes!=test_Y)[0]\n",
    "print(\"Found %d incorrect labels\" % len(incorrect))\n",
    "for i, incorrect in enumerate(incorrect[0:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_X[incorrect].reshape(400,250,3), cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}, {}\".format(deportes[predicted_classes[incorrect]],\n",
    "                                                    deportes[test_Y[incorrect]]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Class {}\".format(i) for i in range(nClasses)]\n",
    "print(classification_report(test_Y, predicted_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_tf=tf.math.confusion_matrix(\n",
    "    test_Y,\n",
    "    predicted_classes,\n",
    "    num_classes=None,\n",
    "    weights=None,\n",
    "    dtype=tf.dtypes.float32,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "con_mat_df = pd.DataFrame(mat_tf,\n",
    "                     index = classes, \n",
    "                     columns = classes)\n",
    "\n",
    "figure = plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred=sport_model.predict(test_X)\n",
    "con_mat = tf.math.confusion_matrix(labels=test_Y, predictions=predicted_classes).numpy()\n",
    "#valid_label\n",
    "#test_Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "con_mat_df = pd.DataFrame(con_mat_norm,\n",
    "                     index = classes, \n",
    "                     columns = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e10f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
